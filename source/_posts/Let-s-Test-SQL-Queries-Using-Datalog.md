---
title: Let's Test SQL Queries Using Datalog
comments: true
tags:
  - Python
  - Datalog
  - SQL
  - Testing
date: 2017-10-16 23:24:49
---



I'm not sure if this is a common practice, but I prefer testing my SQL queries.
As far as I know, there are at least 3 ways to test SQL queries:

1. checking if the query runs on the real data (a.k.a. no testing)
2. providing input data and the expected result (that is calculated manually) and comparing the actual result with the expected one
3. after writing an equivalent query in a non-SQL language (in, e.g., Active Record or datalog), checking outputs from the SQL query and the new query by issuing them onto certain input data

In the past I adopted option 2 mainly. Although we can accumulate test cases by adopting option 2, this way of testing is tedious and incomplete because:

- we have to calculate the output by hand; This is error-prone and time-consuming. (although we may write code for calculating the output, it would require testing for itself...)
- if we change our queries in future, we need to recalculate everything manually
- we cannot test on unknown failure cases; i.e., we cannot use randomly generated cases

If we ask 'can we solve these by using option 3?', the answer is yes since:

- we can generate both outputs (from SQL and some other language) programmatically, rather than manually
- we can adapt to the new case by changing the existing test code, which does not require much calculation by hand
- since both SQL and the language used for testing can process a lot of data (at least much more than human beings can), we can apply randomized testing.

In the following, I'll show you how we can test SQL using [pyDatalog](https://sites.google.com/site/pydatalog/home), a logic programming library in Python.



Installation of Packages
========================

Let's start by installing required packages with [pipenv](https://packaging.python.org/new-tutoriAls/installing-and-using-packages/#installing-pipenv).
To do so, all we need to do is create an empty directory and put `Pipfile` with the following content:

```shell
[[source]]

url = "https://pypi.python.org/simple"
verify_ssl = true
name = "pypi"


[packages]

pydatalog = "*"
ipython = "*"
sqlalchemy = "*"
pandas = "*"


[dev-packages]
```

Then inside the directory, let's run the following command:

```shell
$ pipenv install
```

This should install 4 packages (`pyDatalog`, `pandas`, `ipython` and `sqlalchemy`)and their dependencies.
(In a real environment, you may want to list these in `dev-packages` section.)


Testing Queries with pyDatalog
==============================

In this example, we use sqlite and consider 2 tables generated by the following DDL:

```sql
CREATE TABLE tbl1(
  id integer PRIMARY KEY,
  col1 text,
  created_date text  -- use text since datetime/date does not work well with sqlalchemy
);

```

```sql
CREATE TABLE tbl2(
  id integer PRIMARY KEY,
  tbl1_id integer,
  col2 text
);
```

Here `tbl1_id` column in `tbl2` associates a record in `tbl2` with one or more records in `tbl1`.

Let's create these tables and corresponding datalog terms:

```python
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.ext.declarative import declarative_base

from pyDatalog import pyDatalog

engine = create_engine('sqlite://')

engine.execute("""
CREATE TABLE tbl1(
  id integer PRIMARY KEY,
  col1 text,
  created_date text
);
""")
engine.execute("""
INSERT INTO tbl1(col1, created_date)
VALUES ('foo', '2017-10-11'), ('bar', '2017-10-12');
""")

engine.execute("""
CREATE TABLE tbl2(
  id integer PRIMARY KEY,
  tbl1_id integer,
  col2 text
);
""")
engine.execute("""
INSERT INTO tbl2(tbl1_id, col2)
VALUES (1, 2), (3, 3);
""")


Base = declarative_base(bind=engine, cls=pyDatalog.Mixin, metaclass=pyDatalog.sqlMetaMixin)
Session = sessionmaker(bind=engine)
session = Session()
Base.session = session

class Tbl1(Base):
    __tablename__ = 'tbl1'
    __table_args__ = {'autoload':True}

class Tbl2(Base):
    __tablename__ = 'tbl2'
    __table_args__ = {'autoload':True}
```


Case 1: Parameterized Query
--------------------------

When we write a batch job, we may want to parameterize a query so that we can specify the target date:

```sql
SELECT
  *
FROM
  tbl1
WHERE
  created_date = ?
```

An equivalent query can be written in pyDatalog. We run the following as preparation for the actual query:

```python
import pandas as pd
def get_df(answers):
    """make a pandas dataframe from pyDatalog.Answer.answers field,
    which contains a list of tuples
    """
    return pd.DataFrame([
        dict(id=tup[0].id,
             col1=tup[0].col1,
             created_date=tup[0].created_date) for tup in answers
    ])

@pyDatalog.program()
def parameterized_query_example():
    Tbl1.records_today(X, Today) <= (Tbl1.created_date[X] == Today)
```

If we want to query in pyDatalog, we use `pyDatalog.ask` method:

```python
ans = pyDatalog.ask('Tbl1.records_today(X, "{dt}")'.format(dt='2017-10-12')).answers
by_datalog = get_df(ans)
```

Since we get a result, let's compare it with the result from the sql using [assert_frame_equal](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.testing.assert_frame_equal.html) from pandas:

```
from pandas.testing import assert_frame_equal
sql = """
SELECT
  *
FROM
  tbl1
WHERE
  created_date = ?
"""
by_sql = pd.read_sql(sql, engine, params=('2017-10-12',))
assert_frame_equal(by_sql[sorted(by_sql.columns)], by_datalog[sorted(by_datalog.columns)])
# => passes
```

We successfully tested the SQL!


Case 2: Join
------------

Now let's consider the following query:

```sql
SELECT
  tbl1.id AS id1,
  tbl2.id AS id2
FROM
  tbl1
LEFT OUTER JOIN
  tbl2
ON
  tbl1.id = tbl2.tbl1_id
```

Since we have no record associated with the record with id 2, we need to keep this record in datalog.
We can achieve this in the following manner:

```python
@pyDatalog.program()
def left_join_example():
    has_id1(Z, X) <= (Tbl1.id[X] == Z)
    has_id2(Z, Y) <= (Tbl2.tbl1_id[Y] == Z)
    left_join(Z, X, Y) <= has_id1(Z, X) & has_id2(Z, Y)
    left_join(Z, X, None) <= has_id1(Z, X) & ~has_id2(Z, Y)
```

Let's compare the results from datalog and SQL:

```python
def get_lj_df(answers):
    return pd.DataFrame([
        dict(id1=r1.id, id2=None if r2 is None else r2.id)
        for __, r1, r2 in answers
    ])

by_datalog = get_lj_df(pyDatalog.ask("left_join(Z, X, Y)").answers)

sql2 = """
SELECT
  tbl1.id AS id1,
  tbl2.id AS id2
FROM
  tbl1
LEFT OUTER JOIN
  tbl2
ON
  tbl1.id = tbl2.tbl1_id
"""
by_sql = pd.read_sql(sql2, engine)

## prepare for comparison
by_sql = (
    by_sql[sorted(by_sql.columns)]
    .sort_values(['id1', 'id2'])
    .reset_index(drop=True)
)
by_datalog = (
    by_datalog[sorted(by_datalog.columns)]
    .sort_values(['id1', 'id2'])
    .reset_index(drop=True)
)

assert_frame_equal(by_sql, by_datalog)
# => passes
```

So we can test a query with joins, even if some of them are outer joins!

Conclusion
==========

We can test SQL using datalog and it has the following strength, compared to testing by specifying a pair of input and output:

- it lets us avoid manual calculation
- it lets us use randomized input instances
